---
title: "深度强化学习 李宏毅"
date: 2022-07-03T10:13:53+08:00
---

## MRL Lesson

- Supervised：Learning from teacher
- Reinforcement Learning：Learning from experience

Alpha Go is supervied learning + reinforcement learning.



使用强化学习训练对话机器人：

1. 分别用supervised训练两个agent
2. 让这两个agent进行对话
3. 当它们聊得像人时给与奖励



游戏里面不是本来就有AI吗？为什么还需要用强化学习训练一个AI？

不一样的，强化学习的AI是像人一样训练的，而不是在游戏接口获取信息



**episode**：以游戏为例，一局游戏的开始到结束称为一个episode

> Learn to maximize the expected cumulative reward per episode.



Difficulties of Reinforcement Learning:

- Reward delay
- Agent's actions affect the subsequent data it receives



- Policy-based：Learning an Actor
- Value-based：Learning a Critic

Asynchronous Advantage Actor-Critic（A3C）

Alpha Go：policy-based + value-based + model-based（用于预测未来会发生的动作，围棋中有，游戏中很少）



To learn deep reinforcement：

- Text book: Reinforcement Learning:An Introduction
- Lectures of David Silver
- Lectures of John Schulman



##### Policy-based：Learning an Actor

![image-20220705110146012](深度强化学习 李宏毅.assets/image-20220705110146012.png)

使用Neural network比直接在图像像素上泛化性更好

actor就是Neural network

- actor $\pi_\theta(s)$
  - 输入是$s$，即machine看到的observation

![image-20220705152743412](深度强化学习 李宏毅.assets/image-20220705152743412.png)



![image-20220705155244844](深度强化学习 李宏毅.assets/image-20220705155244844.png)

整个过程：

- 先初始化一个 actor 参数为 $\theta$
- 使用这个 actor 跑若干个 episode
- 使用这些 episode 数据更新model，即得到新的 $\theta$ 的actor
- 然后重复以上过程

![image-20220706080245804](深度强化学习 李宏毅.assets/image-20220706080245804.png)

目标是：调整 Actor 来使最后得到的 Reward 越大越好。



强化学习在80年代就已经很完善了，那为什么现在又火起来了呢？因为之前 Actor 使用的是简单的查表方法，而现在使用的是神经网络。



policy gradient能让我们调 Actor 的参数以使 Reward 最大



![image-20220706085936325](深度强化学习 李宏毅.assets/image-20220706085936325.png)

这个 critic 是给它一个 state ，它会衡量从这个state一直到游戏结束会获得多少 Reward。

![image-20220706092003982](深度强化学习 李宏毅.assets/image-20220706092003982.png)

另一种 critic ，是给它一个 state 和 action，衡量从这个state采取这个action到结束会获得多少 Reward。

Q-learning 的好处，一定能找到一个新 actor 比当前 actor更好。



##### Actor + Critic：A3C

actor 不是看 env 的 reward 而是看 critic 的评价来学习的。

##### Inverse Reinforcement Learning

![image-20220706102119862](深度强化学习 李宏毅.assets/image-20220706102119862.png)

没有 reward funciton，有 demonstration

![image-20220706102540128](深度强化学习 李宏毅.assets/image-20220706102540128.png)

Reinforcement Learning是根据 Reward Function 和 Env 用  Reinforcement Learning 找到最好的 actor

与 Reinforcement Learning 相反，Inverse Reinforcement Learning 不知道最好的 actor是什么，但是有专家做出演示。根据专家的演示和环境通过 Inverse Reinforcement Learning 推出 Reward Function，之后就可以根据这个 Reward Function 找最好的 actor。

![image-20220706103029749](深度强化学习 李宏毅.assets/image-20220706103029749.png)

- 原则：**演示永远是对的**

- 过程

  - 初始化一个`actor`
  - 在每个循环中
    - 让这个`actor`与环境互动，会得到很多过程记录
    - 比较`actor`的过程记录和演示的过程记录，定义一个`Reward Function`让演示得到的分数比`actor`得到的分数高
    - 让`actor`基于这个新的`Reward Function`去最大化奖励
  - 最后输出`Reward Function`和`actor`

  

  

  ![image-20220706103952704](深度强化学习 李宏毅.assets/image-20220706103952704.png)

  这个思路和gan很像

![image-20220706104120535](深度强化学习 李宏毅.assets/image-20220706104120535.png)

## DRL Lesson

### Proximal Policy Optimization

### From on-policy to off-policy

- on-policy：一边玩一边学习
- off-policy：在旁边看别人学

$$
\triangledown \overline {R}_{\theta} = E_{\tau\sim p_{\theta}(\tau)}[R(\tau)\triangledown log {p}_{\theta}(\tau)]
$$

#### Importance Sampling

$E_{x \sim p}[f(x)]$ ：设有个function`f(x)`，计算从第一个`p(x)`sample`x`再把`x`带到`f`

$$
E_{x \sim p}[f(x)]=E_{x \sim q}[f(x)\frac{p(x)}{q(x)}] \\
Var_{x\sim p}[f(x)]  Var_{x\sim q}[f(x)\frac{p(x)}{q(x)}]
$$
问题：

![image-20220712104436526](深度强化学习 李宏毅.assets/image-20220712104436526.png)

当$p(x)$与$q(x)$差距过大时，$V$值差距会很大

![image-20220712104658751](深度强化学习 李宏毅.assets/image-20220712104658751.png)

 ![image-20220712104944277](深度强化学习 李宏毅.assets/image-20220712104944277.png)

要训练 $\pi_\theta$ ，而 $\pi_{\theta '}$ 是用来演示给 $\pi_\theta$ 看的，与环境互动

为什么要换？



![image-20220712105543033](深度强化学习 李宏毅.assets/image-20220712105543033.png)

用 $\theta$ 这个 actor 去 sample 出 $s_t$ 和 $a_t$ ，计算出 $s_t,a_t$ 有多好，$A^\theta$ 是总奖励减去基准值，（估测在$s_t$ 采取 $a_t$ 是好的还是不好的）

有个policy的network，把s_t输进去看a_t概率是多少

![image-20220712144736660](深度强化学习 李宏毅.assets/image-20220712144736660.png)

所以，$p_\theta$ 和 $p_{\theta'}$ 不能相差太多，否则效果会不好，那如何保证不相差太多呢？用PPO和TRPO

 ![image-20220712145429027](深度强化学习 李宏毅.assets/image-20220712145429027.png)

为保证不相差太多，在training时多加一个contrain，这个contrain用来衡量$\theta$和$\theta'$有多像，希望它们的差距越小越好

这个差距指不是参数上的距离，而是行为上的距离（给同样的state后output的action的差距）

![image-20220712152913520](深度强化学习 李宏毅.assets/image-20220712152913520.png)

PPO是放到update里面

PPO使用上比TRPO容易的多

![image-20220712154543568](深度强化学习 李宏毅.assets/image-20220712154543568.png)

![image-20220712154721186](深度强化学习 李宏毅.assets/image-20220712154721186.png)

###  Q-Learning

#### Introduction

vaililiu-based的方法，并不是直接学习policy，而是学习critic

#### Critic

 critic并不是直接采取行为，而是评价现在的行为（actor）有多好

![image-20220712162124491](深度强化学习 李宏毅.assets/image-20220712162124491.png)

critic output值有多大取决于两件事：

- 一个是state
- 另一个是actor

没有能力的actor采取冒险的行为是不好的，而有能力的actor采取冒险的行为反而是好的

![image-20220712162748730](深度强化学习 李宏毅.assets/image-20220712162748730.png)

MC based的方法必须等游戏结束才能计算reward

而TD based的方法不用等到游戏结束，可以在过程中计算奖励

![image-20220713075631210](深度强化学习 李宏毅.assets/image-20220713075631210.png)

例子：

![image-20220713075853596](深度强化学习 李宏毅.assets/image-20220713075853596.png)

![image-20220713075929122](深度强化学习 李宏毅.assets/image-20220713075929122.png)



![image-20220713083601798](深度强化学习 李宏毅.assets/image-20220713083601798.png)

buffer里面装了很多不同policy的data

![image-20220713084243118](深度强化学习 李宏毅.assets/image-20220713084243118.png)

#### Tips

##### Double DQN

Q的值经常被高估，而Double DQN能让估测的值很接近真实值



为什么Q的值经常被高估？

因为我们的目标是让:

![image-20220713084716607](深度强化学习 李宏毅.assets/image-20220713084716607.png)

越接近越好，

而右面target很容易会被设的过高（看哪个a能得到最大的Q值，就把它加上）

![image-20220713090956022](深度强化学习 李宏毅.assets/image-20220713090956022.png)

##### Dueling DQN：只改了network的架构

![image-20220713091120931](深度强化学习 李宏毅.assets/image-20220713091120931.png)



Q-Learning 对于 Continuous Actions处理有困难

